# -*- coding: utf-8 -*-
"""water_meter_imageAlignment.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1uNE0ZYTpDnH-girQI0t8552-umQ-Az_D

step (1) make reference image (inner dial of meter) from water_meter_make_ref.jpg
"""



import cv2
import numpy as np
import matplotlib.pyplot as plt

# === Step 1: Load Image ===
image = cv2.imread("Reference.jpg")

if image is None:
    print("‚ùå Error: Could not load image. Check file path.")
    exit()

# Convert to RGB for display
image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
height, width = image.shape[:2]

print(f"üñºÔ∏è  Image size: {width}x{height}")

# === Step 2: Try Hough Circle Detection ===
print("üîç Attempting Hough Circle Detection for dial...")

# Preprocess: blur and edge detection
blurred = cv2.GaussianBlur(gray, (9, 9), 2)
edges = cv2.Canny(blurred, 50, 150)

# Estimate radius range
min_radius = int(min(width, height) * 0.3)
max_radius = int(min(width, height) * 0.7)

circles = cv2.HoughCircles(
    edges,
    cv2.HOUGH_GRADIENT,
    dp=1.2,
    minDist=height // 4,
    param1=100,
    param2=60,
    minRadius=min_radius,
    maxRadius=max_radius
)

x_center, y_center, r = 0, 0, 0

if circles is not None:
    circles = np.round(circles[0, :]).astype("int")
    print(f"‚úÖ Found {len(circles)} circle(s). Selecting best one...")

    best_circle = None
    best_score = -1

    for (cx, cy, radius) in circles:
        # Score based on size and centrality
        distance_from_center = np.sqrt((cx - width // 2)**2 + (cy - height // 2)**2)
        size_score = radius
        center_score = 1 / (1 + distance_from_center)
        score = size_score * center_score

        if score > best_score:
            best_score = score
            best_circle = (cx, cy, radius)

    x_center, y_center, r = best_circle
    print(f"üéØ Hough-detected: center=({x_center}, {y_center}), radius={r}")

else:
    print("‚ùå No circle detected via Hough. Falling back to darkest pixel method.")

    # Fallback: find darkest pixel in central region
    center_x_guess, center_y_guess = width // 2, height // 2
    search_radius = 150

    top_search = max(center_y_guess - search_radius, 0)
    bottom_search = min(center_y_guess + search_radius, height)
    left_search = max(center_x_guess - search_radius, 0)
    right_search = min(center_x_guess + search_radius, width)

    if top_search >= bottom_search or left_search >= right_search:
        print("‚ùå Search region is invalid. Using image center.")
        x_center, y_center = width // 2, height // 2
    else:
        central_region = gray[top_search:bottom_search, left_search:right_search]
        if central_region.size == 0:
            print("‚ùå Central region is empty. Using image center.")
            x_center, y_center = width // 2, height // 2
        else:
            min_val, max_val, min_loc, max_loc = cv2.minMaxLoc(central_region)
            x_center = left_search + min_loc[0]
            y_center = top_search + min_loc[1]
            print(f"üñ§ Darkest pixel at global: ({x_center}, {y_center})")

    # Set fallback radius
    r = 380
    max_possible_r = min(x_center, width - x_center, y_center, height - y_center)
    if r > max_possible_r:
        print(f"‚ö†Ô∏è Radius {r} too large. Clamping to {max_possible_r - 10}")
        r = max(max_possible_r - 10, 50)

# === Final Validation: Ensure x_center, y_center, r are valid ===
if r <= 0:
    print("‚ùå Radius is invalid (‚â§ 0). Exiting.")
    exit(1)

if not (0 <= x_center < width and 0 <= y_center < height):
    print(f"‚ùå Center ({x_center}, {y_center}) out of image bounds.")
    exit(1)

if np.isnan(x_center) or np.isnan(y_center) or np.isnan(r):
    print("‚ùå NaN detected in center or radius!")
    exit(1)

print(f"‚úÖ Final center: ({x_center}, {y_center}), radius: {r}")

# === Step 3: Create Circular Mask ===
mask = np.zeros((height, width), dtype=np.uint8)
cv2.circle(mask, (int(x_center), int(y_center)), int(r), 255, -1)

# Apply mask: keep inside circle, white outside
cropped_circular = np.full_like(image_rgb, 255)
cropped_circular = np.where(mask[..., None] == 255, image_rgb, cropped_circular)

# === Step 4: Crop to Bounding Box Around Circle ===
top_crop = max(int(y_center - r), 0)
bottom_crop = min(int(y_center + r), height)
left_crop = max(int(x_center - r), 0)
right_crop = min(int(x_center + r), width)

if top_crop >= bottom_crop or left_crop >= right_crop:
    print("‚ùå Crop bounds are invalid (empty region).")
    exit(1)

circular_roi = cropped_circular[top_crop:bottom_crop, left_crop:right_crop]
mask_cropped = mask[top_crop:bottom_crop, left_crop:right_crop]

# === Compute Local Center in Cropped Image ===
local_center_x = x_center - left_crop
local_center_y = y_center - top_crop

# üîí Validate local center
if np.isnan(local_center_x) or np.isnan(local_center_y):
    print(f"‚ùå NaN in local center: ({local_center_x}, {local_center_y})")
    exit(1)

if not isinstance(local_center_x, (int, float, np.integer)) or not isinstance(local_center_y, (int, float, np.integer)):
    print(f"‚ùå Invalid type: local_center_x={type(local_center_x)}, local_center_y={type(local_center_y)}")
    exit(1)


# Convert to int
local_center_x = int(round(float(local_center_x)))
local_center_y = int(round(float(local_center_y)))

# Validate bounds in ROI
if not (0 <= local_center_x < circular_roi.shape[1]):
    print(f"‚ùå local_center_x {local_center_x} out of ROI width {circular_roi.shape[1]}")
    exit(1)
if not (0 <= local_center_y < circular_roi.shape[0]):
    print(f"‚ùå local_center_y {local_center_y} out of ROI height {circular_roi.shape[0]}")
    exit(1)

print(f"üìç Local rotation center: ({local_center_x}, {local_center_y}) in ROI of size {circular_roi.shape[:2]}")

# === Step 5: Rotate by -5¬∞ (Counter-Clockwise) ===
rotation_angle = -5.0  # Negative = CCW

try:
    rotation_matrix = cv2.getRotationMatrix2D((local_center_x, local_center_y), angle=rotation_angle, scale=1.0)
except Exception as e:
    print(f"‚ùå Failed to create rotation matrix: {e}")
    print(f"Center: ({local_center_x}, {local_center_y}), Angle: {rotation_angle}")
    exit(1)

# Apply rotation (white background)
rotated_roi = cv2.warpAffine(
    circular_roi,
    rotation_matrix,
    (circular_roi.shape[1], circular_roi.shape[0]),
    flags=cv2.INTER_CUBIC,
    borderMode=cv2.BORDER_CONSTANT,
    borderValue=(255, 255, 255)
)

# Re-apply mask after rotation to keep perfect circle
mask_rotated = cv2.warpAffine(
    mask_cropped.astype(np.uint8),
    rotation_matrix,
    (mask_cropped.shape[1], mask_cropped.shape[0]),
    flags=cv2.INTER_NEAREST,
    borderMode=cv2.BORDER_CONSTANT,
    borderValue=0
)
rotated_roi = np.where(mask_rotated[..., None] == 255, rotated_roi, 255)

# === Step 6: Save Rotated Full Dial ===
rotated_bgr = cv2.cvtColor(rotated_roi.astype(np.uint8), cv2.COLOR_RGB2BGR)
output_jpg = f"water_meter_circular_crop_rotated_{abs(rotation_angle)}deg_CCW.jpg"
cv2.imwrite(output_jpg, rotated_bgr)
print(f"‚úÖ Rotated image saved as '{output_jpg}'")

# Save transparent version
bgr_cropped_orig = image[top_crop:bottom_crop, left_crop:right_crop]
rgba_rotated_bgr = cv2.warpAffine(
    bgr_cropped_orig, rotation_matrix,
    (bgr_cropped_orig.shape[1], bgr_cropped_orig.shape[0]),
    flags=cv2.INTER_CUBIC,
    borderMode=cv2.BORDER_CONSTANT,
    borderValue=(0, 0, 0)
)
alpha = np.where(mask_cropped == 255, 255, 0).astype(np.uint8)
alpha_rotated = cv2.warpAffine(
    alpha, rotation_matrix,
    (alpha.shape[1], alpha.shape[0]),
    flags=cv2.INTER_NEAREST,
    borderMode=cv2.BORDER_CONSTANT,
    borderValue=0
)
rgba_final = cv2.merge([rgba_rotated_bgr[:, :, 0], rgba_rotated_bgr[:, :, 1], rgba_rotated_bgr[:, :, 2], alpha_rotated])
output_png = f"water_meter_circular_crop_rotated_{abs(rotation_angle)}deg_CCW_transparent.png"
cv2.imwrite(output_png, rgba_final)
print(f"‚úÖ Transparent PNG saved as '{output_png}'")

# === Step 7: Display Rotated + Original ===
fig, axes = plt.subplots(1, 3, figsize=(18, 6))

# 1. Original with detected circle
img_with_circle = image_rgb.copy()
cv2.circle(img_with_circle, (int(x_center), int(y_center)), int(r), (255, 0, 0), 10)
cv2.circle(img_with_circle, (int(x_center), int(y_center)), 7, (255, 255, 255), -1)
cv2.circle(img_with_circle, (int(x_center), int(y_center)), 5, (255, 0, 0), -1)
axes[0].imshow(img_with_circle)
axes[0].set_title(f"Detected Dial\nCenter ({x_center}, {y_center}), r={r}")
axes[0].axis('off')

# 2. Before rotation
axes[1].imshow(circular_roi)
axes[1].set_title("Cropped ROI (Before Rotation)")
axes[1].axis('off')

# 3. After rotation
axes[2].imshow(rotated_roi)
axes[2].set_title(f"After {abs(rotation_angle)}¬∞ CCW Rotation")
axes[2].axis('off')

plt.tight_layout()
plt.show()

# === Step 8: Extract Inner Circle ROI (Digit Ring) ===
print("üü¢ Extracting inner circle ROI...")

# --- Parameters ---
inner_radius_ratio = 0.7  # Adjust: 0.6‚Äì0.8 depending on digit position
inner_r = int(r * inner_radius_ratio)
print(f"Inner radius: {inner_r} pixels ({inner_radius_ratio*100:.0f}% of full radius)")

center_inner = (local_center_x, local_center_y)  # Same center in rotated image

# Create mask for inner circle
mask_inner = np.zeros(rotated_roi.shape[:2], dtype=np.uint8)
cv2.circle(mask_inner, center_inner, inner_r, 255, -1)

# Apply mask: keep inner, white background
inner_roi = np.full_like(rotated_roi, 255)
inner_roi = np.where(mask_inner[..., None] == 255, rotated_roi, inner_roi)

# Crop to bounding box
top_inner = max(center_inner[1] - inner_r, 0)
bottom_inner = min(center_inner[1] + inner_r, rotated_roi.shape[0])
left_inner = max(center_inner[0] - inner_r, 0)
right_inner = min(center_inner[0] + inner_r, rotated_roi.shape[1])

if top_inner >= bottom_inner or left_inner >= right_inner:
    print("‚ùå Inner crop bounds invalid.")
else:
    inner_cropped = inner_roi[top_inner:bottom_inner, left_inner:right_inner]
    mask_inner_cropped = mask_inner[top_inner:bottom_inner, left_inner:right_inner]
    inner_cropped = np.where(mask_inner_cropped[..., None] == 255, inner_cropped, 255)

    # Save inner ROI
    inner_bgr = cv2.cvtColor(inner_cropped.astype(np.uint8), cv2.COLOR_RGB2BGR)
    cv2.imwrite("water_meter_inner_digit_ring.jpg", inner_bgr)
    print("‚úÖ Inner circle ROI saved as 'water_meter_inner_digit_ring.jpg'")

    # Save transparent
    alpha_inner = np.where(mask_inner_cropped == 255, 255, 0).astype(np.uint8)
    inner_rgba = cv2.merge([inner_bgr[:, :, 0], inner_bgr[:, :, 1], inner_bgr[:, :, 2], alpha_inner])
    cv2.imwrite("water_meter_inner_digit_ring_transparent.png", inner_rgba)
    cv2.imwrite("reference.jpg", inner_rgba)
    print("‚úÖ Transparent inner ROI saved as 'water_meter_inner_digit_ring_transparent.png'")

    # Display
    plt.figure(figsize=(12, 6))
    plt.subplot(1, 2, 1)
    img_debug = rotated_roi.copy()
    cv2.circle(img_debug, center_inner, inner_r, (0, 255, 0), 3)  # Green outline
    plt.imshow(img_debug)
    plt.title("Rotated Dial with Inner Circle (Green)")
    plt.axis('off')

    plt.subplot(1, 2, 2)
    plt.imshow(inner_cropped)
    plt.title(f"Inner Circle ROI (r = {inner_r})")
    plt.axis('off')

    plt.tight_layout()
    plt.show()

import cv2
import numpy as np
import matplotlib.pyplot as plt
import os, glob

# === CONFIG ===
input_folder = r"D:\projectCPE\dataset\images\test"
output_folder = r"D:\projectCPE\dataset\output"
os.makedirs(output_folder, exist_ok=True)

rotation_angle = -5.0  # ‡∏´‡∏°‡∏∏‡∏ô‡∏ó‡∏ß‡∏ô‡πÄ‡∏Ç‡πá‡∏°‡πÄ‡∏õ‡πá‡∏ô‡∏≠‡∏á‡∏®‡∏≤

# === Loop through all images ===
image_files = glob.glob(os.path.join(input_folder, "*.jpg"))
if not image_files:
    print("‚ùå No images found in folder:", input_folder)
    exit()

for img_path in image_files:
    print("\n=== Processing:", img_path, "===")
    image = cv2.imread(img_path)

    if image is None:
        print("‚ùå Error: Could not load image.")
        continue

    # Convert to RGB for display
    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)
    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
    height, width = image.shape[:2]
    print(f"üñºÔ∏è  Image size: {width}x{height}")

    # === Step 1: Hough Circle Detection ===
    blurred = cv2.GaussianBlur(gray, (9, 9), 2)
    edges = cv2.Canny(blurred, 50, 150)

    min_radius = int(min(width, height) * 0.3)
    max_radius = int(min(width, height) * 0.7)

    circles = cv2.HoughCircles(
        edges, cv2.HOUGH_GRADIENT, dp=1.2, minDist=height // 4,
        param1=100, param2=60,
        minRadius=min_radius, maxRadius=max_radius
    )

    x_center, y_center, r = 0, 0, 0

    if circles is not None:
        circles = np.round(circles[0, :]).astype("int")
        best_circle = None
        best_score = -1
        for (cx, cy, radius) in circles:
            distance_from_center = np.sqrt((cx - width // 2)**2 + (cy - height // 2)**2)
            size_score = radius
            center_score = 1 / (1 + distance_from_center)
            score = size_score * center_score
            if score > best_score:
                best_score = score
                best_circle = (cx, cy, radius)
        x_center, y_center, r = best_circle
        print(f"üéØ Hough-detected: center=({x_center}, {y_center}), radius={r}")
    else:
        # fallback: use image center
        x_center, y_center = width // 2, height // 2
        r = min(width, height) // 2
        print(f"‚ö†Ô∏è Hough failed. Using center=({x_center},{y_center}), r={r}")

    # === Step 2: Create Circular Mask and Crop ===
    mask = np.zeros((height, width), dtype=np.uint8)
    cv2.circle(mask, (int(x_center), int(y_center)), int(r), 255, -1)

    cropped_circular = np.full_like(image_rgb, 255)
    cropped_circular = np.where(mask[..., None] == 255, image_rgb, cropped_circular)

    top_crop = max(int(y_center - r), 0)
    bottom_crop = min(int(y_center + r), height)
    left_crop = max(int(x_center - r), 0)
    right_crop = min(int(x_center + r), width)
    circular_roi = cropped_circular[top_crop:bottom_crop, left_crop:right_crop]
    mask_cropped = mask[top_crop:bottom_crop, left_crop:right_crop]

    # Local center in cropped ROI
    local_center_x = int(x_center - left_crop)
    local_center_y = int(y_center - top_crop)
    print(f"üìç Local rotation center: ({local_center_x}, {local_center_y})")

    # === Step 3: Rotate ROI ===
    rotation_matrix = cv2.getRotationMatrix2D((local_center_x, local_center_y), angle=rotation_angle, scale=1.0)
    rotated_roi = cv2.warpAffine(
        circular_roi, rotation_matrix,
        (circular_roi.shape[1], circular_roi.shape[0]),
        flags=cv2.INTER_CUBIC, borderMode=cv2.BORDER_CONSTANT, borderValue=(255, 255, 255)
    )

    # Re-apply mask to keep circular region
    mask_rotated = cv2.warpAffine(mask_cropped, rotation_matrix,
                                  (mask_cropped.shape[1], mask_cropped.shape[0]),
                                  flags=cv2.INTER_NEAREST,
                                  borderMode=cv2.BORDER_CONSTANT, borderValue=0)
    rotated_roi = np.where(mask_rotated[..., None] == 255, rotated_roi, 255)

    # === Step 4: Save Results ===
    base_name = os.path.splitext(os.path.basename(img_path))[0]
    output_jpg = os.path.join(output_folder, f"{base_name}_rotated.jpg")
    cv2.imwrite(output_jpg, cv2.cvtColor(rotated_roi, cv2.COLOR_RGB2BGR))
    print("‚úÖ Saved:", output_jpg)

    # Save Transparent PNG
    alpha = np.where(mask_rotated == 255, 255, 0).astype(np.uint8)
    rgba_final = cv2.merge([rotated_roi[:, :, 0], rotated_roi[:, :, 1], rotated_roi[:, :, 2], alpha])
    output_png = os.path.join(output_folder, f"{base_name}_rotated.png")
    cv2.imwrite(output_png, cv2.cvtColor(rgba_final, cv2.COLOR_RGBA2BGRA))
    print("‚úÖ Saved transparent:", output_png)

print("\nüéâ All images processed!")

"""step (2) this part is used to alightment the test_water_meter.jpg (e.g. image from espcam)

you can change test_image at line 409 and results will be saved in alignment_results folder
"""

import cv2
import numpy as np
import os
from pathlib import Path
import matplotlib.pyplot as plt
import matplotlib.patches as patches
from matplotlib.gridspec import GridSpec
import warnings
warnings.filterwarnings("ignore", message="Glyph.*missing from font.*")

class WaterMeterAligner:
    def __init__(self):
        """Initialize the Water Meter Aligner with optimized settings"""
        # Use SIFT for better accuracy with meter details
        self.detector = cv2.SIFT_create(nfeatures=8000)
        self.good_match_percent = 0.20

    def preprocess_meter_image(self, image):
        """Preprocess meter images for better feature detection"""
        # Convert to grayscale
        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) if len(image.shape) == 3 else image

        # Apply CLAHE for better contrast on numbers and text
        clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))
        enhanced = clahe.apply(gray)

        # Slight gaussian blur to reduce noise
        blurred = cv2.GaussianBlur(enhanced, (3, 3), 0)

        return blurred

    def detect_and_compute(self, image):
        """Detect keypoints and compute descriptors with preprocessing"""
        processed = self.preprocess_meter_image(image)
        keypoints, descriptors = self.detector.detectAndCompute(processed, None)
        return keypoints, descriptors

    def match_features(self, desc1, desc2):
        """Match features between two images"""
        if desc1 is None or desc2 is None:
            return []

        # Use BFMatcher for SIFT descriptors
        matcher = cv2.BFMatcher(cv2.NORM_L2, crossCheck=True)
        matches = matcher.match(desc1, desc2)
        matches = sorted(matches, key=lambda x: x.distance)

        # Keep only good matches
        num_good_matches = int(len(matches) * self.good_match_percent)
        return matches[:num_good_matches]

    def detect_circular_features(self, image):
        """Detect circular features for validation"""
        gray = self.preprocess_meter_image(image)

        # Detect circles using HoughCircles
        circles = cv2.HoughCircles(gray, cv2.HOUGH_GRADIENT, dp=1, minDist=50,
                                 param1=50, param2=30, minRadius=50, maxRadius=300)

        return circles

    def align_meter_images(self, reference_img, image_to_align):
        """
        Align water meter images with quality assessment

        Returns:
            aligned_image: The aligned image
            homography: The transformation matrix
            match_count: Number of good matches found
            quality_score: Quality score (0-1)
        """
        print("Detecting features in reference image...")
        kp1, desc1 = self.detect_and_compute(reference_img)
        print(f"Found {len(kp1)} keypoints in reference image")

        print("Detecting features in image to align...")
        kp2, desc2 = self.detect_and_compute(image_to_align)
        print(f"Found {len(kp2)} keypoints in image to align")

        # Match features
        print("Matching features...")
        matches = self.match_features(desc1, desc2)
        print(f"Found {len(matches)} good matches")

        if len(matches) < 4:
            print(f"ERROR: Only {len(matches)} matches found. Need at least 4 for homography.")
            return image_to_align, None, len(matches), 0.0

        # Extract matched keypoints
        points1 = np.float32([kp1[m.queryIdx].pt for m in matches]).reshape(-1, 1, 2)
        points2 = np.float32([kp2[m.trainIdx].pt for m in matches]).reshape(-1, 1, 2)

        # Find homography with strict parameters
        print("Computing homography transformation...")
        homography, mask = cv2.findHomography(points2, points1,
                                            cv2.RANSAC,
                                            ransacReprojThreshold=3.0,
                                            maxIters=5000,
                                            confidence=0.995)

        if homography is None:
            print("ERROR: Could not find valid homography.")
            return image_to_align, None, len(matches), 0.0

        # Calculate alignment quality based on inliers
        inliers = np.sum(mask) if mask is not None else 0
        quality_score = inliers / len(matches) if len(matches) > 0 else 0
        print(f"Alignment quality: {quality_score:.2f} ({inliers}/{len(matches)} inliers)")

        # Apply homography to align the image
        height, width = reference_img.shape[:2]
        aligned_image = cv2.warpPerspective(image_to_align, homography, (width, height))

        # Validate using circular features
        print("Validating circular alignment...")
        ref_circles = self.detect_circular_features(reference_img)
        aligned_circles = self.detect_circular_features(aligned_image)

        if ref_circles is not None and aligned_circles is not None:
            ref_center = (ref_circles[0][0][0], ref_circles[0][0][1])
            aligned_center = (aligned_circles[0][0][0], aligned_circles[0][0][1])
            center_distance = np.sqrt((ref_center[0] - aligned_center[0])**2 +
                                    (ref_center[1] - aligned_center[1])**2)

            print(f"Circular alignment error: {center_distance:.1f} pixels")

            # Adjust quality score based on circular alignment
            if center_distance < 20:
                quality_score = min(1.0, quality_score * 1.2)
                print("‚úì Excellent circular alignment")
            elif center_distance > 50:
                quality_score *= 0.8
                print("‚ö† Poor circular alignment")
            else:
                print("‚úì Good circular alignment")

        return aligned_image, homography, len(matches), quality_score

    def create_comparison_plot(self, reference_img, original_img, aligned_img,
                              match_count, quality_score, save_path=None):
        """
        Create a comprehensive comparison plot with multiple views
        """
        # Convert BGR to RGB for matplotlib
        ref_rgb = cv2.cvtColor(reference_img, cv2.COLOR_BGR2RGB)
        orig_rgb = cv2.cvtColor(original_img, cv2.COLOR_BGR2RGB)
        aligned_rgb = cv2.cvtColor(aligned_img, cv2.COLOR_BGR2RGB)

        # Create figure with custom layout
        fig = plt.figure(figsize=(20, 12))
        gs = GridSpec(3, 4, figure=fig, height_ratios=[2, 2, 1], width_ratios=[1, 1, 1, 1])

        # Main comparison row
        ax1 = fig.add_subplot(gs[0, 0])
        ax2 = fig.add_subplot(gs[0, 1])
        ax3 = fig.add_subplot(gs[0, 2])
        ax4 = fig.add_subplot(gs[0, 3])

        # Display main images
        ax1.imshow(ref_rgb)
        ax1.set_title('REFERENCE IMAGE', fontsize=14, fontweight='bold', color='green')
        ax1.axis('off')

        ax2.imshow(orig_rgb)
        ax2.set_title('ORIGINAL IMAGE', fontsize=14, fontweight='bold', color='red')
        ax2.axis('off')

        ax3.imshow(aligned_rgb)
        ax3.set_title('ALIGNED IMAGE', fontsize=14, fontweight='bold', color='blue')
        ax3.axis('off')

        # Create difference image
        diff_img = cv2.absdiff(ref_rgb, aligned_rgb)
        diff_gray = cv2.cvtColor(diff_img, cv2.COLOR_RGB2GRAY)
        ax4.imshow(diff_gray, cmap='hot')
        ax4.set_title('ALIGNMENT DIFFERENCE', fontsize=14, fontweight='bold', color='purple')
        ax4.axis('off')

        # Zoomed comparison row (center regions)
        ax5 = fig.add_subplot(gs[1, 0])
        ax6 = fig.add_subplot(gs[1, 1])
        ax7 = fig.add_subplot(gs[1, 2])
        ax8 = fig.add_subplot(gs[1, 3])

        # Extract center regions for detailed comparison
        h, w = ref_rgb.shape[:2]
        crop_size = min(h, w) // 3
        y_start, x_start = h//2 - crop_size//2, w//2 - crop_size//2
        y_end, x_end = y_start + crop_size, x_start + crop_size

        ref_crop = ref_rgb[y_start:y_end, x_start:x_end]
        orig_crop = orig_rgb[y_start:y_end, x_start:x_end]
        aligned_crop = aligned_rgb[y_start:y_end, x_start:x_end]
        diff_crop = diff_gray[y_start:y_end, x_start:x_end]

        ax5.imshow(ref_crop)
        ax5.set_title('Reference (Center)', fontsize=12, color='green')
        ax5.axis('off')

        ax6.imshow(orig_crop)
        ax6.set_title('Original (Center)', fontsize=12, color='red')
        ax6.axis('off')

        ax7.imshow(aligned_crop)
        ax7.set_title('Aligned (Center)', fontsize=12, color='blue')
        ax7.axis('off')

        ax8.imshow(diff_crop, cmap='hot')
        ax8.set_title('Difference (Center)', fontsize=12, color='purple')
        ax8.axis('off')

        # Add crop rectangles to main images
        for ax in [ax1, ax2, ax3, ax4]:
            rect = patches.Rectangle((x_start, y_start), crop_size, crop_size,
                                   linewidth=2, edgecolor='yellow', facecolor='none', linestyle='--')
            ax.add_patch(rect)

        # Statistics and metrics row
        ax9 = fig.add_subplot(gs[2, :])
        ax9.axis('off')

        # Create metrics text
        metrics_text = f"""
ALIGNMENT METRICS AND STATISTICS

Feature Matches: {match_count} good matches found
Quality Score: {quality_score:.3f} / 1.000

Quality Assessment: """

        if quality_score > 0.8:
            quality_text = "üü¢ EXCELLENT - High confidence alignment"
            quality_color = 'green'
        elif quality_score > 0.6:
            quality_text = "üü° GOOD - Reliable alignment"
            quality_color = 'orange'
        elif quality_score > 0.4:
            quality_text = "üü† FAIR - Acceptable alignment"
            quality_color = 'darkorange'
        else:
            quality_text = "üî¥ POOR - Manual review recommended"
            quality_color = 'red'

        # Calculate additional metrics
        mse = np.mean((ref_rgb.astype(float) - aligned_rgb.astype(float)) ** 2)
        psnr = 20 * np.log10(255.0 / np.sqrt(mse)) if mse > 0 else float('inf')

        full_metrics_text = f"""{metrics_text}{quality_text}

Technical Metrics:
‚Ä¢ Mean Squared Error (MSE): {mse:.2f}
‚Ä¢ Peak Signal-to-Noise Ratio (PSNR): {psnr:.2f} dB
‚Ä¢ Image Dimensions: {h} x {w} pixels
‚Ä¢ Center Crop Size: {crop_size} x {crop_size} pixels

Color Legend:
üü¢ Green = Reference Image    üî¥ Red = Original Image    üîµ Blue = Aligned Result    üü£ Purple = Difference Map
"""

        ax9.text(0.05, 0.5, full_metrics_text, transform=ax9.transAxes, fontsize=11,
                verticalalignment='center', fontfamily='monospace',
                bbox=dict(boxstyle="round,pad=0.5", facecolor="lightgray", alpha=0.8))

        # Add quality indicator
        ax9.text(0.75, 0.7, quality_text, transform=ax9.transAxes, fontsize=14,
                fontweight='bold', color=quality_color,
                bbox=dict(boxstyle="round,pad=0.3", facecolor="white", edgecolor=quality_color, linewidth=2))

        # Overall title
        fig.suptitle('WATER METER ALIGNMENT ANALYSIS', fontsize=18, fontweight='bold', y=0.95)

        plt.tight_layout()

        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight', facecolor='white')
            print(f"‚úì Detailed comparison plot saved: {save_path}")

        return fig

    def create_feature_matching_plot(self, reference_img, image_to_align, save_path=None):
        """
        Create an enhanced feature matching visualization
        """
        # Get features and matches
        kp1, desc1 = self.detect_and_compute(reference_img)
        kp2, desc2 = self.detect_and_compute(image_to_align)
        matches = self.match_features(desc1, desc2)

        # Convert to RGB
        ref_rgb = cv2.cvtColor(reference_img, cv2.COLOR_BGR2RGB)
        img_rgb = cv2.cvtColor(image_to_align, cv2.COLOR_BGR2RGB)

        # Create figure
        fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(20, 15))

        # Top: Individual images with keypoints
        ax1.imshow(ref_rgb)
        ax1.set_title(f'Reference Image - {len(kp1)} Keypoints Detected', fontsize=14, fontweight='bold')

        # Draw keypoints on reference
        for kp in kp1[:200]:  # Show top 200 keypoints
            circle = patches.Circle((kp.pt[0], kp.pt[1]), radius=3,
                                  facecolor='red', edgecolor='yellow', alpha=0.7)
            ax1.add_patch(circle)
        ax1.axis('off')

        # Middle: Test image with keypoints
        ax2.imshow(img_rgb)
        ax2.set_title(f'Test Image - {len(kp2)} Keypoints Detected', fontsize=14, fontweight='bold')

        # Draw keypoints on test image
        for kp in kp2[:200]:  # Show top 200 keypoints
            circle = patches.Circle((kp.pt[0], kp.pt[1]), radius=3,
                                  facecolor='blue', edgecolor='cyan', alpha=0.7)
            ax2.add_patch(circle)
        ax2.axis('off')

        # Bottom: Feature matches
        # Create side-by-side image for matches
        h1, w1 = ref_rgb.shape[:2]
        h2, w2 = img_rgb.shape[:2]
        max_height = max(h1, h2)

        # Resize images to same height
        ref_resized = cv2.resize(ref_rgb, (int(w1 * max_height / h1), max_height))
        img_resized = cv2.resize(img_rgb, (int(w2 * max_height / h2), max_height))

        # Combine images side by side
        combined = np.hstack([ref_resized, img_resized])
        ax3.imshow(combined)

        # Draw matches
        matches_to_show = min(50, len(matches))  # Show top 50 matches
        colors = plt.cm.rainbow(np.linspace(0, 1, matches_to_show))

        for i, match in enumerate(matches[:matches_to_show]):
            # Get matched keypoints
            pt1 = kp1[match.queryIdx].pt
            pt2 = kp2[match.trainIdx].pt

            # Adjust pt2 coordinates for combined image
            pt2_adjusted = (pt2[0] + ref_resized.shape[1], pt2[1] * max_height / h2)
            pt1_adjusted = (pt1[0] * max_height / h1, pt1[1] * max_height / h1)

            # Draw line connecting matched points
            ax3.plot([pt1_adjusted[0], pt2_adjusted[0]],
                    [pt1_adjusted[1], pt2_adjusted[1]],
                    color=colors[i], linewidth=1, alpha=0.7)

            # Draw circles at matched points
            circle1 = patches.Circle(pt1_adjusted, radius=4,
                                   facecolor=colors[i], edgecolor='white', alpha=0.8)
            circle2 = patches.Circle(pt2_adjusted, radius=4,
                                   facecolor=colors[i], edgecolor='white', alpha=0.8)
            ax3.add_patch(circle1)
            ax3.add_patch(circle2)

        ax3.set_title(f'Feature Matches - Top {matches_to_show} of {len(matches)} Total Matches',
                     fontsize=14, fontweight='bold')
        ax3.axis('off')

        # Add legend
        legend_text = f"""
Feature Detection Summary:
‚Ä¢ Reference keypoints: {len(kp1)}
‚Ä¢ Test image keypoints: {len(kp2)}
‚Ä¢ Total matches found: {len(matches)}
‚Ä¢ Matches displayed: {matches_to_show}
‚Ä¢ Match quality: {len(matches)/min(len(kp1), len(kp2)):.1%}

Legend: üî¥ Reference keypoints, üîµ Test keypoints, üåà Matched pairs
"""

        ax3.text(0.02, 0.98, legend_text, transform=ax3.transAxes, fontsize=10,
                verticalalignment='top', bbox=dict(boxstyle="round,pad=0.5",
                facecolor="white", alpha=0.9))

        plt.tight_layout()

        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight', facecolor='white')
            print(f"‚úì Feature matching plot saved: {save_path}")

        return fig

def demo_water_meter_alignment():
    """
    Demo function that aligns water_meter.jpg to reference image in current folder
    """
    print("=== Water Meter Alignment Demo ===\n")

    # Look for reference image in current folder
    reference_files = ['reference.jpg', 'ref.jpg', 'reference_meter.jpg', 'water_meter_ref.jpg']
    reference_path = None

    for ref_file in reference_files:
        if os.path.exists(ref_file):
            reference_path = ref_file
            break

    if reference_path is None:
        print("ERROR: No reference image found!")
        print("Please place one of these files in the current folder:")
        for ref_file in reference_files:
            print(f"  - {ref_file}")
        return

    # Check for test image
    test_image_path = "test_water_meter.jpg"
    if not os.path.exists(test_image_path):
        print(f"ERROR: Test image '{test_image_path}' not found in current folder!")
        print("Please place your test image in the current folder.")
        return

    print(f"‚úì Reference image: {reference_path}")
    print(f"‚úì Test image: {test_image_path}")
    print()

      # Load images
    print("Loading images...")
    reference_img = cv2.imread('Reference.jpg')
    test_img = cv2.imread(test_image_path)

    if reference_img is None:
        print(f"ERROR: Could not load reference image: {reference_path}")
        return

    if test_img is None:
        print(f"ERROR: Could not load test image: {test_image_path}")
        return

    print(f"Reference image size: {reference_img.shape[:2]}")
    print(f"Test image size: {test_img.shape[:2]}")
    print()

    # --- ‡∏õ‡∏£‡∏±‡∏ö‡∏Ç‡∏ô‡∏≤‡∏î‡∏†‡∏≤‡∏û test ‡πÄ‡∏õ‡πá‡∏ô 750x750 ---
    TARGET_SIZE = (750, 750)
    test_img = cv2.resize(test_img, TARGET_SIZE, interpolation=cv2.INTER_AREA)
    print(f"Resized test image size: {test_img.shape[:2]}")
    print()

    # Initialize aligner
    aligner = WaterMeterAligner()

    # Perform alignment
    print("Starting alignment process...")
    print("-" * 50)
    aligned_img, homography, match_count, quality = aligner.align_meter_images(
        reference_img, test_img)
    print("-" * 50)

    # Save results
    output_dir = "alignment_results"
    os.makedirs(output_dir, exist_ok=True)

    # Save aligned image
    aligned_path = os.path.join(output_dir, "aligned_water_meter.jpg")
    cv2.imwrite(aligned_path, aligned_img)
    print(f"‚úì Aligned image saved: {aligned_path}")

    # Save transformation matrix
    if homography is not None:
        transform_path = os.path.join(output_dir, "transformation_matrix.npy")
        np.save(transform_path, homography)
        print(f"‚úì Transformation matrix saved: {transform_path}")

    # Create and save comparison image
    print("Creating detailed comparison visualization...")

    # Create comprehensive comparison plot
    comparison_fig = aligner.create_comparison_plot(
        reference_img, test_img, aligned_img, match_count, quality)

    comparison_path = os.path.join(output_dir, "detailed_comparison.png")
    comparison_fig.savefig(comparison_path, dpi=300, bbox_inches='tight', facecolor='white')
    plt.close(comparison_fig)  # Free memory
    print(f"‚úì Detailed comparison plot saved: {comparison_path}")

    # Create feature matching visualization
    print("Creating enhanced feature matching visualization...")
    matching_fig = aligner.create_feature_matching_plot(reference_img, test_img)
    matches_path = os.path.join(output_dir, "enhanced_feature_matches.png")
    matching_fig.savefig(matches_path, dpi=300, bbox_inches='tight', facecolor='white')
    plt.close(matching_fig)  # Free memory
    print(f"‚úì Enhanced feature matching plot saved: {matches_path}")

    # Also create simple OpenCV comparison for reference
    print("Creating simple OpenCV comparison...")

    # Resize images to same height for simple comparison
    h1, w1 = reference_img.shape[:2]
    h2, w2 = test_img.shape[:2]
    h3, w3 = aligned_img.shape[:2]

    max_height = max(h1, h2, h3)

    # Resize maintaining aspect ratio
    ref_resized = cv2.resize(reference_img, (int(w1 * max_height / h1), max_height))
    test_resized = cv2.resize(test_img, (int(w2 * max_height / h2), max_height))
    aligned_resized = cv2.resize(aligned_img, (int(w3 * max_height / h3), max_height))

    # Create side-by-side comparison
    comparison = np.hstack([ref_resized, test_resized, aligned_resized])

    # Add labels
    label_height = 50
    labeled_comparison = np.zeros((comparison.shape[0] + label_height, comparison.shape[1], 3), dtype=np.uint8)
    labeled_comparison[label_height:, :] = comparison

    # Add text labels
    cv2.putText(labeled_comparison, 'REFERENCE', (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)
    cv2.putText(labeled_comparison, 'ORIGINAL', (ref_resized.shape[1] + 10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2)
    cv2.putText(labeled_comparison, 'ALIGNED', (ref_resized.shape[1] + test_resized.shape[1] + 10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 0, 0), 2)

    opencv_comparison_path = os.path.join(output_dir, "simple_comparison.jpg")
    cv2.imwrite(opencv_comparison_path, labeled_comparison)
    print(f"‚úì Simple OpenCV comparison saved: {opencv_comparison_path}")

    # Create basic feature matches using OpenCV
    kp1, desc1 = aligner.detect_and_compute(reference_img)
    kp2, desc2 = aligner.detect_and_compute(test_img)
    matches = aligner.match_features(desc1, desc2)

    basic_match_img = cv2.drawMatches(reference_img, kp1, test_img, kp2,
                                     matches[:100], None,
                                     flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)
    basic_matches_path = os.path.join(output_dir, "basic_feature_matches.jpg")
    cv2.imwrite(basic_matches_path, basic_match_img)
    print(f"‚úì Basic feature matches saved: {basic_matches_path}")

    # Print summary
    print("\n" + "=" * 50)
    print("ALIGNMENT SUMMARY")
    print("=" * 50)
    print(f"Feature matches found: {match_count}")
    print(f"Alignment quality: {quality:.2f}/1.00")

    if quality > 0.8:
        print("üü¢ EXCELLENT alignment quality!")
    elif quality > 0.6:
        print("üü° GOOD alignment quality")
    elif quality > 0.4:
        print("üü† FAIR alignment quality")
    else:
        print("üî¥ POOR alignment quality - check your images")

    print(f"\nResults saved in '{output_dir}' folder:")
    print(f"  üì∏ aligned_water_meter.jpg (final aligned image)")
    print(f"  üìä detailed_comparison.png (comprehensive analysis plot)")
    print(f"  üîó enhanced_feature_matches.png (detailed feature matching)")
    print(f"  üìã simple_comparison.jpg (basic side-by-side view)")
    print(f"  üéØ basic_feature_matches.jpg (OpenCV feature matches)")
    print(f"  üî¢ transformation_matrix.npy (transformation data)")

    print(f"\nüí° TIP: Open 'detailed_comparison.png' for the best analysis view!")

    return aligned_img, quality

def main():
    """Main function to run the demo"""
    try:
        demo_water_meter_alignment()
    except Exception as e:
        print(f"ERROR: {str(e)}")
        print("\nTroubleshooting tips:")
        print("1. Make sure you have a reference image (reference.jpg, ref.jpg, etc.)")
        print("2. Make sure you have a test image named 'water_meter.jpg'")
        print("3. Install required packages: pip install opencv-python numpy matplotlib")
        print("4. Check that your images are valid and not corrupted")

if __name__ == "__main__":
    print("Water Meter Auto-Alignment Demo")
    print("This script will align 'water_meter.jpg' to your reference image")
    print("Make sure both images are in the current folder!\n")

    main()

def check_keypoints(reference_path, test_path):
    aligner = WaterMeterAligner()

    # ‡πÇ‡∏´‡∏•‡∏î‡∏†‡∏≤‡∏û
    ref_img = cv2.imread(reference_path)
    test_img = cv2.imread(test_path)

    if ref_img is None or test_img is None:
        print(" ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÇ‡∏´‡∏•‡∏î‡∏†‡∏≤‡∏û‡πÑ‡∏î‡πâ")
        return

    # ‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö keypoints
    kp_ref, desc_ref = aligner.detect_and_compute(ref_img)
    kp_test, desc_test = aligner.detect_and_compute(test_img)

    print(f"üîπ Reference keypoints: {len(kp_ref)}")
    print(f"üîπ Test keypoints: {len(kp_test)}")

    # ‡∏à‡∏±‡∏ö‡∏Ñ‡∏π‡πà keypoints
    matches = aligner.match_features(desc_ref, desc_test)
    print(f"üîπ Matches found: {len(matches)}")

    # ‡∏ó‡∏≥ alignment
    aligned_img, homography, match_count, quality = aligner.align_meter_images(ref_img, test_img)


    print(f"‚úÖ Alignment done, quality = {quality:.2f}")
    return aligned_img, kp_ref, kp_test, matches


def check_keypoints(reference_path, test_path):
    aligner = WaterMeterAligner()

    # ‡πÇ‡∏´‡∏•‡∏î‡∏†‡∏≤‡∏û
    ref_img = cv2.imread(reference_path)
    test_img = cv2.imread(test_path)

    if ref_img is None or test_img is None:
        print("‚ùå ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÇ‡∏´‡∏•‡∏î‡∏†‡∏≤‡∏û‡πÑ‡∏î‡πâ")
        return

    # ‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö keypoints
    kp_ref, desc_ref = aligner.detect_and_compute(ref_img)
    kp_test, desc_test = aligner.detect_and_compute(test_img)

    print(f"üîπ Reference keypoints: {len(kp_ref)}")
    print(f"üîπ Test keypoints: {len(kp_test)}")

    # ‡∏à‡∏±‡∏ö‡∏Ñ‡∏π‡πà keypoints
    matches = aligner.match_features(desc_ref, desc_test)
    print(f"üîπ Matches found: {len(matches)}")

    # ‡∏ó‡∏≥ alignment
    aligned_img, homography, match_count, quality = aligner.align_meter_images(ref_img, test_img)

    print(f"‚úÖ Alignment done, quality = {quality:.2f}")
    return aligned_img, kp_ref, kp_test, matches
